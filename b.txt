============================= test session starts ==============================
platform darwin -- Python 3.12.6, pytest-8.3.2, pluggy-1.5.0
rootdir: /Users/kadelin/Desktop/Cornell/MLE/mod3-cin-cout
configfile: pyproject.toml
plugins: hypothesis-6.54.0, env-1.1.4
collected 53 items / 2 deselected / 51 selected

tests/test_tensor_general.py .F.FF..F.FFFFFFFF.FFFF.FF..F.FFFFFFFF...FFF [ 84%]
FFF...FF                                                                 [100%]

=================================== FAILURES ===================================
___________________________ test_one_args[fast-fn0] ____________________________

fn = ('addConstant', <function MathTest.addConstant at 0x110023ba0>, <function MathTest.addConstant at 0x110023ba0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(5.0), b = np.float64(6.0)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=5.000000 y=6.000000
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('addConstant', addConstant, addConstant),
E           backend='fast',
E       )
E       Draw 1:
E       [0.00 1.00]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn2] ____________________________

fn = ('cube', <function MathTest.cube at 0x110023ce0>, <function MathTest.cube at 0x110023ce0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.0), b = np.float64(1.0)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.000000 y=1.000000
E       Falsifying example: test_one_args(
E           data=data(...), fn=('cube', cube, cube), backend='fast',
E       )
E       Draw 1:
E       [
E       	[0.00 1.00]]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn3] ____________________________

fn = ('div', <function MathTest.div at 0x110023ec0>, <function MathTest.div at 0x110023ec0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.0), b = np.float64(0.2)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.000000 y=0.200000
E       Falsifying example: test_one_args(
E           data=data(...), fn=('div', div, div), backend='fast',
E       )
E       Draw 1:
E       [0.00 1.00]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn6] ____________________________

fn = ('inv', <function MathTest.inv at 0x110023f60>, <function MathTestVariable.inv at 0x110034ae0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.2857142857142857), b = np.float64(0.2222222222222222)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.285714 y=0.222222
E       Falsifying example: test_one_args(
E           data=data(...), fn=('inv', inv, inv), backend='fast',
E       )
E       Draw 1:
E       [0.00 1.00]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn8] ____________________________

fn = ('multConstant', <function MathTest.multConstant at 0x110023e20>, <function MathTest.multConstant at 0x110023e20>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.0), b = np.float64(5.0)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.000000 y=5.000000
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('multConstant', multConstant, multConstant),
E           backend='fast',
E       )
E       Draw 1:
E       [0.00 1.00]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn9] ____________________________

fn = ('neg', <function MathTest.neg at 0x110013240>, <function MathTest.neg at 0x110013240>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(-0.0), b = np.float64(-1.0)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=-0.000000 y=-1.000000
E       Falsifying example: test_one_args(
E           data=data(...), fn=('neg', neg, neg), backend='fast',
E       )
E       Draw 1:
E       [
E       	[
E       		[0.00 1.00]]]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn10] ___________________________

fn = ('relu', <function MathTest.relu at 0x110034180>, <function MathTestVariable.relu at 0x110034cc0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(5.5), b = np.float64(6.5)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=5.500000 y=6.500000
E       Falsifying example: test_one_args(
E           data=data(...), fn=('relu', relu, relu), backend='fast',
E       )
E       Draw 1:
E       [0.00 1.00]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn11] ___________________________

fn = ('sig', <function MathTest.sig at 0x110034040>, <function MathTestVariable.sig at 0x110034b80>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.5), b = 0.7310585786300049

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.500000 y=0.731059
E       Falsifying example: test_one_args(
E           data=data(...), fn=('sig', sig, sig), backend='fast',
E       )
E       Draw 1:
E       [
E       	[0.00 1.00]]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn12] ___________________________

fn = ('square', <function MathTest.square at 0x110023c40>, <function MathTest.square at 0x110023c40>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.0), b = np.float64(1.0)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.000000 y=1.000000
E       Falsifying example: test_one_args(
E           data=data(...), fn=('square', square, square), backend='fast',
E       )
E       Draw 1:
E       [
E       	[0.00 1.00]]

tests/strategies.py:17: AssertionError
___________________________ test_one_args[fast-fn13] ___________________________

fn = ('subConstant', <function MathTest.subConstant at 0x110023d80>, <function MathTest.subConstant at 0x110023d80>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:55:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:68: in test_one_args
    assert_close(t2[ind], base_fn(t1[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(-5.0), b = np.float64(-4.0)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=-5.000000 y=-4.000000
E       Falsifying example: test_one_args(
E           data=data(...),
E           fn=('subConstant', subConstant, subConstant),
E           backend='fast',
E       )
E       Draw 1:
E       [0.00 1.00]

tests/strategies.py:17: AssertionError
___________________________ test_two_args[fast-fn0] ____________________________

fn = ('add2', <function MathTest.add2 at 0x110034360>, <function MathTest.add2 at 0x110034360>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:72:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:85: in test_two_args
    assert_close(t3[ind], base_fn(t1[ind], t2[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.0), b = np.float64(1.0)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.000000 y=1.000000
E       Falsifying example: test_two_args(
E           data=data(...), fn=('add2', add2, add2), backend='fast',
E       )
E       Draw 1: [
E       [
E       	[0.00 0.00]],
E       [
E       	[0.00 1.00]]]

tests/strategies.py:17: AssertionError
___________________________ test_two_args[fast-fn1] ____________________________

fn = ('div2', <function MathTest.div2 at 0x1100344a0>, <function MathTest.div2 at 0x1100344a0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:72:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:85: in test_two_args
    assert_close(t3[ind], base_fn(t1[ind], t2[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.18181818181818182), b = np.float64(0.15384615384615385)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.181818 y=0.153846
E       Falsifying example: test_two_args(
E           data=data(...), fn=('div2', div2, div2), backend='fast',
E       )
E       Draw 1: [
E       [
E       	[0.00 1.00]],
E       [
E       	[0.00 1.00]]]

tests/strategies.py:17: AssertionError
___________________________ test_two_args[fast-fn3] ____________________________

fn = ('gt2', <function MathTest.gt2 at 0x110034540>, <function MathTestVariable.gt2 at 0x110035120>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:72:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:85: in test_two_args
    assert_close(t3[ind], base_fn(t1[ind], t2[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(1.0), b = 0.0

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=1.000000 y=0.000000
E       Falsifying example: test_two_args(
E           data=data(...), fn=('gt2', gt2, gt2), backend='fast',
E       )
E       Draw 1: [
E       [
E       	[0.00 0.00]],
E       [
E       	[0.00 2.00]]]

tests/strategies.py:17: AssertionError
___________________________ test_two_args[fast-fn4] ____________________________

fn = ('lt2', <function MathTest.lt2 at 0x1100345e0>, <function MathTestVariable.lt2 at 0x1100351c0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:72:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:85: in test_two_args
    assert_close(t3[ind], base_fn(t1[ind], t2[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.0), b = 1.0

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.000000 y=1.000000
E       Falsifying example: test_two_args(
E           data=data(...), fn=('lt2', lt2, lt2), backend='fast',
E       )
E       Draw 1: [
E       [
E       	[0.00 0.00]],
E       [
E       	[0.00 2.00]]]

tests/strategies.py:17: AssertionError
___________________________ test_two_args[fast-fn5] ____________________________

fn = ('mul2', <function MathTest.mul2 at 0x110034400>, <function MathTest.mul2 at 0x110034400>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:72:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:85: in test_two_args
    assert_close(t3[ind], base_fn(t1[ind], t2[ind]))
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

a = np.float64(0.0), b = np.float64(1.0)

    def assert_close(a: float, b: float) -> None:
>       assert minitorch.operators.is_close(a, b), "Failure x=%f y=%f" % (a, b)
E       AssertionError: Failure x=0.000000 y=1.000000
E       Falsifying example: test_two_args(
E           data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E       )
E       Draw 1: [
E       [
E       	[0.00 1.00]],
E       [
E       	[0.00 1.00]]]

tests/strategies.py:17: AssertionError
________________________ test_one_derivative[fast-fn0] _________________________

fn = ('addConstant', <function MathTest.addConstant at 0x110023ba0>, <function MathTest.addConstant at 0x110023ba0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x111de4cc0>, array(1.), array(2.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.addConstant at 0x110023ba0>.\... 2.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.addConstant at 0x110023ba0>.
E
E           Input (
E           [0.00 0.00],)
E
E           Received derivative 1.000000 for argument 0 and index (0,),
E           but was expecting derivative 2.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: 0.5
E            ACTUAL: array(1.)
E            DESIRED: array(2.)
E           Falsifying example: test_one_derivative(
E               data=data(...),
E               fn=('addConstant', addConstant, addConstant),
E               backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn2] _________________________

fn = ('cube', <function MathTest.cube at 0x110023ce0>, <function MathTest.cube at 0x110023ce0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x111ddf7e0>, array(0.), array(3.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.cube at 0x110023ce0>.\n\nInpu... 3.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.cube at 0x110023ce0>.
E
E           Input (
E           [0.00 0.00 1.00],)
E
E           Received derivative 0.000000 for argument 0 and index (2,),
E           but was expecting derivative 3.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 3.
E           Max relative difference among violations: 1.
E            ACTUAL: array(0.)
E            DESIRED: array(3.)
E           Falsifying example: test_one_derivative(
E               data=data(...), fn=('cube', cube, cube), backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00 1.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn3] _________________________

fn = ('div', <function MathTest.div at 0x110023ec0>, <function MathTest.div at 0x110023ec0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1113979c0>, array(0.2), array(0.4))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.div at 0x110023ec0>.\n\nInput... 0.400000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.div at 0x110023ec0>.
E
E           Input (
E           [0.00 0.00],)
E
E           Received derivative 0.200000 for argument 0 and index (0,),
E           but was expecting derivative 0.400000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 0.2
E           Max relative difference among violations: 0.5
E            ACTUAL: array(0.2)
E            DESIRED: array(0.4)
E           Falsifying example: test_one_derivative(
E               data=data(...), fn=('div', div, div), backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn6] _________________________

fn = ('inv', <function MathTest.inv at 0x110023f60>, <function MathTestVariable.inv at 0x110034ae0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1113b71a0>, array(-0.08163265), array(-0.16326531))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTestVariable.inv at 0x110034ae0>.\...-0.163265 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTestVariable.inv at 0x110034ae0>.
E
E           Input (
E           [0.00 0.00],)
E
E           Received derivative -0.081633 for argument 0 and index (0,),
E           but was expecting derivative -0.163265 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 0.08163265
E           Max relative difference among violations: 0.5
E            ACTUAL: array(-0.081633)
E            DESIRED: array(-0.163265)
E           Falsifying example: test_one_derivative(
E               data=data(...), fn=('inv', inv, inv), backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn8] _________________________

fn = ('multConstant', <function MathTest.multConstant at 0x110023e20>, <function MathTest.multConstant at 0x110023e20>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1111a6c00>, array(5.), array(10.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.multConstant at 0x110023e20>....10.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.multConstant at 0x110023e20>.
E
E           Input (
E           [0.00 0.00],)
E
E           Received derivative 5.000000 for argument 0 and index (0,),
E           but was expecting derivative 10.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 5.
E           Max relative difference among violations: 0.5
E            ACTUAL: array(5.)
E            DESIRED: array(10.)
E           Falsifying example: test_one_derivative(
E               data=data(...),
E               fn=('multConstant', multConstant, multConstant),
E               backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn9] _________________________

fn = ('neg', <function MathTest.neg at 0x110013240>, <function MathTest.neg at 0x110013240>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1116dc220>, array(-1.), array(0.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.neg at 0x110013240>.\n\nInput... 0.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.neg at 0x110013240>.
E
E           Input (
E           [
E           	[0.00 0.00]],)
E
E           Received derivative -1.000000 for argument 0 and index (0, 1),
E           but was expecting derivative 0.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: inf
E            ACTUAL: array(-1.)
E            DESIRED: array(0.)
E           Falsifying example: test_one_derivative(
E               data=data(...), fn=('neg', neg, neg), backend='fast',
E           )
E           Draw 1:
E           [
E           	[0.00 0.00]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn10] ________________________

fn = ('relu', <function MathTest.relu at 0x110034180>, <function MathTestVariable.relu at 0x110034cc0>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1117b5ee0>, array(1.), array(2.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTestVariable.relu at 0x110034cc0>.... 2.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTestVariable.relu at 0x110034cc0>.
E
E           Input (
E           [0.00 0.00],)
E
E           Received derivative 1.000000 for argument 0 and index (0,),
E           but was expecting derivative 2.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: 0.5
E            ACTUAL: array(1.)
E            DESIRED: array(2.)
E           Falsifying example: test_one_derivative(
E               data=data(...), fn=('relu', relu, relu), backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn11] ________________________

fn = ('sig', <function MathTest.sig at 0x110034040>, <function MathTestVariable.sig at 0x110034b80>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x111e2d940>, array(0.25), array(0.19661193))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTestVariable.sig at 0x110034b80>.\... 0.196612 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTestVariable.sig at 0x110034b80>.
E
E           Input (
E           [0.00 0.00 1.00],)
E
E           Received derivative 0.250000 for argument 0 and index (2,),
E           but was expecting derivative 0.196612 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 0.05338807
E           Max relative difference among violations: 0.27154032
E            ACTUAL: array(0.25)
E            DESIRED: array(0.196612)
E           Falsifying example: test_one_derivative(
E               data=data(...), fn=('sig', sig, sig), backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00 1.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn12] ________________________

fn = ('square', <function MathTest.square at 0x110023c40>, <function MathTest.square at 0x110023c40>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x11137db20>, array(1.), array(2.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.square at 0x110023c40>.\n\nIn... 2.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.square at 0x110023c40>.
E
E           Input (
E           [0.00 0.00 1.00],)
E
E           Received derivative 1.000000 for argument 0 and index (2,),
E           but was expecting derivative 2.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: 0.5
E            ACTUAL: array(1.)
E            DESIRED: array(2.)
E           Falsifying example: test_one_derivative(
E               data=data(...), fn=('square', square, square), backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00 1.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
________________________ test_one_derivative[fast-fn13] ________________________

fn = ('subConstant', <function MathTest.subConstant at 0x110023d80>, <function MathTest.subConstant at 0x110023d80>)
backend = 'fast'

    @given(data())
>   @pytest.mark.parametrize("fn", one_arg)

tests/test_tensor_general.py:89:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:99: in test_one_derivative
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x11137d620>, array(1.), array(2.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.subConstant at 0x110023d80>.\... 2.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.subConstant at 0x110023d80>.
E
E           Input (
E           [0.00 0.00],)
E
E           Received derivative 1.000000 for argument 0 and index (0,),
E           but was expecting derivative 2.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: 0.5
E            ACTUAL: array(1.)
E            DESIRED: array(2.)
E           Falsifying example: test_one_derivative(
E               data=data(...),
E               fn=('subConstant', subConstant, subConstant),
E               backend='fast',
E           )
E           Draw 1:
E           [0.00 0.00]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
___________________________ test_two_grad[fast-fn0] ____________________________

fn = ('add2', <function MathTest.add2 at 0x110034360>, <function MathTest.add2 at 0x110034360>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:103:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:114: in test_two_grad
    grad_check(tensor_fn, t1, t2)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x111e2eac0>, array(1.), array(0.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.add2 at 0x110034360>.\n\nInpu... 0.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.add2 at 0x110034360>.
E
E           Input (
E           [
E           	[0.00 0.00]],
E           [
E           	[0.00 0.00]])
E
E           Received derivative 1.000000 for argument 0 and index (0, 1),
E           but was expecting derivative 0.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: inf
E            ACTUAL: array(1.)
E            DESIRED: array(0.)
E           Falsifying example: test_two_grad(
E               data=data(...), fn=('add2', add2, add2), backend='fast',
E           )
E           Draw 1: [
E           [
E           	[0.00 0.00]],
E           [
E           	[0.00 0.00]]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
___________________________ test_two_grad[fast-fn1] ____________________________

fn = ('div2', <function MathTest.div2 at 0x1100344a0>, <function MathTest.div2 at 0x1100344a0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:103:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:114: in test_two_grad
    grad_check(tensor_fn, t1, t2)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x11137cf40>, array(0.), array(-0.03305785))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.div2 at 0x1100344a0>.\n\nInpu...-0.033058 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.div2 at 0x1100344a0>.
E
E           Input (
E           [
E           	[0.00 1.00]],
E           [
E           	[0.00 0.00]])
E
E           Received derivative 0.000000 for argument 1 and index (0, 0),
E           but was expecting derivative -0.033058 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 0.03305785
E           Max relative difference among violations: 1.
E            ACTUAL: array(0.)
E            DESIRED: array(-0.033058)
E           Falsifying example: test_two_grad(
E               data=data(...), fn=('div2', div2, div2), backend='fast',
E           )
E           Draw 1: [
E           [
E           	[0.00 1.00]],
E           [
E           	[0.00 0.00]]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
___________________________ test_two_grad[fast-fn5] ____________________________

fn = ('mul2', <function MathTest.mul2 at 0x110034400>, <function MathTest.mul2 at 0x110034400>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=50)

tests/test_tensor_general.py:103:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:114: in test_two_grad
    grad_check(tensor_fn, t1, t2)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1116b1580>, array(0.), array(1.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.mul2 at 0x110034400>.\n\nInpu... 1.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.mul2 at 0x110034400>.
E
E           Input (
E           [
E           	[0.00 0.00]],
E           [
E           	[0.00 1.00]])
E
E           Received derivative 0.000000 for argument 0 and index (0, 1),
E           but was expecting derivative 1.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: 1.
E            ACTUAL: array(0.)
E            DESIRED: array(1.)
E           Falsifying example: test_two_grad(
E               data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E           )
E           Draw 1: [
E           [
E           	[0.00 0.00]],
E           [
E           	[0.00 1.00]]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
____________________________ test_reduce[fast-fn0] _____________________________

fn = ('mean_full_red', <function MathTest.mean_full_red at 0x110034860>, <function MathTestVariable.mean_full_red at 0x110034fe0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:118:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:129: in test_reduce
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1113b4f40>, array(0.5), array(0.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTestVariable.mean_full_red at 0x11... 0.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTestVariable.mean_full_red at 0x110034fe0>.
E
E           Input (
E           [
E           	[0.00 0.00]],)
E
E           Received derivative 0.500000 for argument 0 and index (0, 1),
E           but was expecting derivative 0.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 0.5
E           Max relative difference among violations: inf
E            ACTUAL: array(0.5)
E            DESIRED: array(0.)
E           Falsifying example: test_reduce(
E               data=data(...),
E               fn=('mean_full_red', mean_full_red, mean_full_red),
E               backend='fast',
E           )
E           Draw 1:
E           [
E           	[0.00 0.00]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
____________________________ test_reduce[fast-fn1] _____________________________

fn = ('mean_red', <function MathTest.mean_red at 0x1100347c0>, <function MathTestVariable.mean_red at 0x110034f40>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:118:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:129: in test_reduce
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1113942c0>, array(1.), array(0.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTestVariable.mean_red at 0x110034f... 0.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTestVariable.mean_red at 0x110034f40>.
E
E           Input (
E           [
E           	[0.00 0.00]],)
E
E           Received derivative 1.000000 for argument 0 and index (0, 1),
E           but was expecting derivative 0.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: inf
E            ACTUAL: array(1.)
E            DESIRED: array(0.)
E           Falsifying example: test_reduce(
E               data=data(...), fn=('mean_red', mean_red, mean_red), backend='fast',
E           )
E           Draw 1:
E           [
E           	[0.00 0.00]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
____________________________ test_reduce[fast-fn2] _____________________________

fn = ('sum_red', <function MathTest.sum_red at 0x110034720>, <function MathTestVariable.sum_red at 0x110034ea0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:118:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:129: in test_reduce
    grad_check(tensor_fn, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x11137c900>, array(1.), array(0.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTestVariable.sum_red at 0x110034ea... 0.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTestVariable.sum_red at 0x110034ea0>.
E
E           Input (
E           [
E           	[0.00 0.00]],)
E
E           Received derivative 1.000000 for argument 0 and index (0, 1),
E           but was expecting derivative 0.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: inf
E            ACTUAL: array(1.)
E            DESIRED: array(0.)
E           Falsifying example: test_reduce(
E               data=data(...), fn=('sum_red', sum_red, sum_red), backend='fast',
E           )
E           Draw 1:
E           [
E           	[0.00 0.00]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
______________________ test_two_grad_broadcast[fast-fn0] _______________________

fn = ('add2', <function MathTest.add2 at 0x110034360>, <function MathTest.add2 at 0x110034360>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:309:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:324: in test_two_grad_broadcast
    grad_check(tensor_fn, t1.sum(0), t2)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x111035760>, array(1.), array(0.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.add2 at 0x110034360>.\n\nInpu... 0.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.add2 at 0x110034360>.
E
E           Input (
E           [0.00],
E           [0.00 0.00])
E
E           Received derivative 1.000000 for argument 1 and index (1,),
E           but was expecting derivative 0.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: inf
E            ACTUAL: array(1.)
E            DESIRED: array(0.)
E           Falsifying example: test_two_grad_broadcast(
E               data=data(...), fn=('add2', add2, add2), backend='fast',
E           )
E           Draw 1: [
E           [0.00 0.00],
E           [0.00 0.00]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
______________________ test_two_grad_broadcast[fast-fn1] _______________________

fn = ('div2', <function MathTest.div2 at 0x1100344a0>, <function MathTest.div2 at 0x1100344a0>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:309:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:325: in test_two_grad_broadcast
    grad_check(tensor_fn, t1, t2.sum(0))
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1111a4ea0>, array(0.18181818), array(0.36363636))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.div2 at 0x1100344a0>.\n\nInpu... 0.363636 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.div2 at 0x1100344a0>.
E
E           Input (
E           [0.00 0.00],
E           [0.00])
E
E           Received derivative 0.181818 for argument 0 and index (0,),
E           but was expecting derivative 0.363636 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 0.18181818
E           Max relative difference among violations: 0.5
E            ACTUAL: array(0.181818)
E            DESIRED: array(0.363636)
E           Falsifying example: test_two_grad_broadcast(
E               data=data(...), fn=('div2', div2, div2), backend='fast',
E           )
E           Draw 1: [
E           [0.00 0.00],
E           [0.00 0.00]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
______________________ test_two_grad_broadcast[fast-fn5] _______________________

fn = ('mul2', <function MathTest.mul2 at 0x110034400>, <function MathTest.mul2 at 0x110034400>)
backend = 'fast'

    @given(data())
>   @settings(max_examples=25)

tests/test_tensor_general.py:309:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:321: in test_two_grad_broadcast
    grad_check(tensor_fn, t1, t2)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x1111a7740>, array(0.), array(1.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function MathTest.mul2 at 0x110034400>.\n\nInpu... 1.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function MathTest.mul2 at 0x110034400>.
E
E           Input (
E           [
E           	[0.00 0.00]],
E           [
E           	[0.00 1.00]])
E
E           Received derivative 0.000000 for argument 0 and index (0, 1),
E           but was expecting derivative 1.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: 1.
E            ACTUAL: array(0.)
E            DESIRED: array(1.)
E           Falsifying example: test_two_grad_broadcast(
E               data=data(...), fn=('mul2', mul2, mul2), backend='fast',
E           )
E           Draw 1: [
E           [
E           	[0.00 0.00]],
E           [
E           	[0.00 1.00]]]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
______________________________ test_permute[fast] ______________________________

backend = 'fast'

    @given(data())
>   @settings(max_examples=100)

tests/test_tensor_general.py:329:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _
tests/test_tensor_general.py:339: in test_permute
    minitorch.grad_check(permute, t1)
minitorch/tensor_functions.py:878: in grad_check
    np.testing.assert_allclose(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

args = (<function assert_allclose.<locals>.compare at 0x111e2dd00>, array(1.), array(0.))
kwds = {'equal_nan': True, 'err_msg': '\n\nGradient check error for function <function test_permute.<locals>.permute at 0x111... 0.000000 from central difference.\n\n', 'header': 'Not equal to tolerance rtol=0.01, atol=0.01', 'strict': False, ...}

    @wraps(func)
    def inner(*args, **kwds):
        with self._recreate_cm():
>           return func(*args, **kwds)
E           AssertionError:
E           Not equal to tolerance rtol=0.01, atol=0.01
E
E
E           Gradient check error for function <function test_permute.<locals>.permute at 0x111de7600>.
E
E           Input (
E           [
E           	[0.00 0.00]],)
E
E           Received derivative 1.000000 for argument 0 and index (0, 1),
E           but was expecting derivative 0.000000 from central difference.
E
E
E           Mismatched elements: 1 / 1 (100%)
E           Max absolute difference among violations: 1.
E           Max relative difference among violations: inf
E            ACTUAL: array(1.)
E            DESIRED: array(0.)
E           Falsifying example: test_permute(
E               data=data(...), backend='fast',
E           )
E           Draw 1:
E           [
E           	[0.00 0.00]]
E           Draw 2: [1, 0]

/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81: AssertionError
=========================== short test summary info ============================
FAILED tests/test_tensor_general.py::test_one_args[fast-fn0] - AssertionError...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn2] - AssertionError...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn3] - AssertionError...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn6] - AssertionError...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn8] - AssertionError...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn9] - AssertionError...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn10] - AssertionErro...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn11] - AssertionErro...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn12] - AssertionErro...
FAILED tests/test_tensor_general.py::test_one_args[fast-fn13] - AssertionErro...
FAILED tests/test_tensor_general.py::test_two_args[fast-fn0] - AssertionError...
FAILED tests/test_tensor_general.py::test_two_args[fast-fn1] - AssertionError...
FAILED tests/test_tensor_general.py::test_two_args[fast-fn3] - AssertionError...
FAILED tests/test_tensor_general.py::test_two_args[fast-fn4] - AssertionError...
FAILED tests/test_tensor_general.py::test_two_args[fast-fn5] - AssertionError...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn0] - Assertio...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn2] - Assertio...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn3] - Assertio...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn6] - Assertio...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn8] - Assertio...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn9] - Assertio...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn10] - Asserti...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn11] - Asserti...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn12] - Asserti...
FAILED tests/test_tensor_general.py::test_one_derivative[fast-fn13] - Asserti...
FAILED tests/test_tensor_general.py::test_two_grad[fast-fn0] - AssertionError:
FAILED tests/test_tensor_general.py::test_two_grad[fast-fn1] - AssertionError:
FAILED tests/test_tensor_general.py::test_two_grad[fast-fn5] - AssertionError:
FAILED tests/test_tensor_general.py::test_reduce[fast-fn0] - AssertionError:
FAILED tests/test_tensor_general.py::test_reduce[fast-fn1] - AssertionError:
FAILED tests/test_tensor_general.py::test_reduce[fast-fn2] - AssertionError:
FAILED tests/test_tensor_general.py::test_two_grad_broadcast[fast-fn0] - Asse...
FAILED tests/test_tensor_general.py::test_two_grad_broadcast[fast-fn1] - Asse...
FAILED tests/test_tensor_general.py::test_two_grad_broadcast[fast-fn5] - Asse...
FAILED tests/test_tensor_general.py::test_permute[fast] - AssertionError:
================= 35 failed, 16 passed, 2 deselected in 19.48s =================
